{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LD5WkI-cr4f"
      },
      "outputs": [],
      "source": [
        "# vqa_clip_gpt2.py\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_models():\n",
        "    # Load CLIP for image+text encoding\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    # Load GPT-2 for text generation\n",
        "    gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "    return clip_model, clip_processor, gpt_model, gpt_tokenizer\n",
        "\n",
        "def preprocess_image(url):\n",
        "    image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "def generate_answer(image, question, clip_model, clip_processor, gpt_model, gpt_tokenizer):\n",
        "    inputs = clip_processor(text=[question], images=image, return_tensors=\"pt\", padding=True)\n",
        "    outputs = clip_model(**inputs)\n",
        "\n",
        "    text_features = outputs.text_embeds\n",
        "    image_features = outputs.image_embeds\n",
        "\n",
        "    # Simple fusion (for demo): concatenate image and text embeddings\n",
        "    context_vector = (image_features + text_features) / 2\n",
        "\n",
        "    # Prompt GPT-2 with question and force answer generation\n",
        "    prompt = f\"Q: {question}\\nA:\"\n",
        "    input_ids = gpt_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    output = gpt_model.generate(input_ids, max_new_tokens=50, pad_token_id=gpt_tokenizer.eos_token_id)\n",
        "    answer = gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer.split(\"A:\")[-1].strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Loading models...\")\n",
        "    clip_model, clip_processor, gpt_model, gpt_tokenizer = load_models()\n",
        "\n",
        "    # Sample image and question\n",
        "    image_url = \"https://raw.githubusercontent.com/zhoubolei/zhou/images/visual_question.jpg\"\n",
        "    image = preprocess_image(image_url)\n",
        "    question = \"What is the man doing?\"\n",
        "\n",
        "    print(\"Generating answer...\")\n",
        "    answer = generate_answer(image, question, clip_model, clip_processor, gpt_model, gpt_tokenizer)\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}